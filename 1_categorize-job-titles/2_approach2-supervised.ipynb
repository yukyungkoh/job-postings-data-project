{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d037058a-8828-470d-ad11-20191e5a7d13",
   "metadata": {},
   "source": [
    "# 🏷️ Part 1.2 – Categorizing Job Titles Using Supervised Learning\n",
    "\n",
    "**Author:** Yu Kyung Koh  \n",
    "**Last Updated:** May 18, 2025\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Objective\n",
    "\n",
    "* This notebook applies **supervised machine learning** methods—including logistic regression and multinomial Naive Bayes—to classify job titles into occupational categories.  \n",
    "* In supervised learning, a model is trained on labeled examples (in this case, job titles with known categories) to learn patterns that can generalize to new, unseen titles.\n",
    "* This approach is useful when manually labeled data is available and when scalable, automated classification is needed across large job posting datasets.\n",
    "* Note that job category labels are known by design, as they were assigned during the LLM-based generation of synthetic job postings ([details here](https://github.com/yukyungkoh/job-postings-data-project/tree/main/0_generate-synthetic-job-postings)). For testing, we treat the categories as unknown and evaluate model predictions.\n",
    "---\n",
    "\n",
    "### 🗂️ Outline\n",
    "\n",
    "- **Section 1:** Prepare the job posting data  \n",
    "- **Section 2:** Train-test split  \n",
    "- **Section 3:** Feature extraction using TF-IDF  \n",
    "- **Section 4:** Method 1: Logistic regression  \n",
    "- **Section 5:** Method 2: Multinomial Naive Bayes\n",
    "- **Section 6:** Method 3: Support Vector Machines (SVM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcf4a30-c074-480f-bd52-a14fa50a0650",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 1: Prepare the job posting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60407a75-6cdc-4f55-aeb0-0178ff089664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4448a51-fda5-4e74-80e0-b7a176a1b53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       job_id                             company_name  \\\n",
      "0      921716                    Corcoran Sawyer Smith   \n",
      "2    10998357                   The National Exemplar    \n",
      "12   56482768                                      NaN   \n",
      "14   69333422                          Staffing Theory   \n",
      "18  111513530  United Methodists of Greater New Jersey   \n",
      "\n",
      "                                            title  work_type  \\\n",
      "0                           Marketing Coordinator  FULL_TIME   \n",
      "2                     Assitant Restaurant Manager  FULL_TIME   \n",
      "12  Appalachian Highlands Women's Business Center  FULL_TIME   \n",
      "14               Senior Product Marketing Manager  FULL_TIME   \n",
      "18                 Content Writer, Communications  FULL_TIME   \n",
      "\n",
      "    normalized_salary                                      combined_desc  \\\n",
      "0             38480.0  Job descriptionA leading real estate firm in N...   \n",
      "2             55000.0  The National Exemplar is accepting application...   \n",
      "12                NaN  FULL JOB DESCRIPTION – PROGRAM DIRECTOR Appala...   \n",
      "14                NaN  A leading pharmaceutical company committed to ...   \n",
      "18                NaN  Application opening date: April 24, 2024\\nTitl...   \n",
      "\n",
      "            job_category  \n",
      "0              Marketing  \n",
      "2          Other Manager  \n",
      "12  Business/Finance Job  \n",
      "14             Marketing  \n",
      "18             Marketing  \n"
     ]
    }
   ],
   "source": [
    "## Import cleaned data from code 1-1\n",
    "cleandatadir = '/Users/yukyungkoh/Desktop/1_Post-PhD/7_Python-projects/2_practice-NLP_job-posting_NEW/2_data/cleaned_data'\n",
    "jobdata = os.path.join(cleandatadir, '1_job-posting_jobs-categorized_df.pkl')\n",
    "jobs_df = pd.read_pickle(jobdata, 'zip')\n",
    "print(jobs_df.head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3dd51de-901b-434b-97ef-bd7eb2760bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing\n",
    "jobs_df = jobs_df.dropna(subset=['combined_desc', 'job_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad3208c0-6ea6-47b3-bc88-23a3787d6644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       job_id                             company_name  \\\n",
      "0      921716                    Corcoran Sawyer Smith   \n",
      "2    10998357                   The National Exemplar    \n",
      "12   56482768                                      NaN   \n",
      "14   69333422                          Staffing Theory   \n",
      "18  111513530  United Methodists of Greater New Jersey   \n",
      "\n",
      "                                            title  work_type  \\\n",
      "0                           Marketing Coordinator  FULL_TIME   \n",
      "2                     Assitant Restaurant Manager  FULL_TIME   \n",
      "12  Appalachian Highlands Women's Business Center  FULL_TIME   \n",
      "14               Senior Product Marketing Manager  FULL_TIME   \n",
      "18                 Content Writer, Communications  FULL_TIME   \n",
      "\n",
      "    normalized_salary                                      combined_desc  \\\n",
      "0             38480.0  Job descriptionA leading real estate firm in N...   \n",
      "2             55000.0  The National Exemplar is accepting application...   \n",
      "12                NaN  FULL JOB DESCRIPTION – PROGRAM DIRECTOR Appala...   \n",
      "14                NaN  A leading pharmaceutical company committed to ...   \n",
      "18                NaN  Application opening date: April 24, 2024\\nTitl...   \n",
      "\n",
      "            job_category  category_encoded  \n",
      "0              Marketing                 3  \n",
      "2          Other Manager                 4  \n",
      "12  Business/Finance Job                 0  \n",
      "14             Marketing                 3  \n",
      "18             Marketing                 3  \n"
     ]
    }
   ],
   "source": [
    "# Encode target labels\n",
    "#  => This convert the category labels from text to numeric form, which is required by most scikit-learn models \n",
    "\n",
    "le = LabelEncoder()\n",
    "jobs_df['category_encoded'] = le.fit_transform(jobs_df['job_category'])\n",
    "print(jobs_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8662f18-b7b7-4fc6-8c14-ddeb9a767e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_encoded\n",
      "4    13385\n",
      "0     4557\n",
      "5     3400\n",
      "3     2598\n",
      "6     1994\n",
      "2     1946\n",
      "1     1844\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Preview category\n",
    "print(jobs_df[\"category_encoded\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "132b87ae-fb80-4254-8915-869c5ab062a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine job title and job description \n",
    "\n",
    "jobs_df['all_text'] = jobs_df['title'].fillna('') + ' ' + jobs_df['combined_desc'].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21cb5ad-9fda-4111-8d49-0725c76bf2ee",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 2: Train-test split\n",
    "\n",
    "* In this section, I split the dataset into training and test sets for supervised learning.  \n",
    "* The `all_text` column -- containing job title and description -- is used as the input feature, and the `category_encoded` column serves as the target variable.  \n",
    "* I use stratified sampling to ensure that the distribution of job categories is preserved across both the training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16929824-3199-4651-9fca-794f5208b311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    jobs_df['all_text'], \n",
    "    jobs_df['category_encoded'], \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=jobs_df['category_encoded']  ## Keeps category proportions the same in train/test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4767c8cc-262c-4624-9fdb-ef8e5668f53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train ## Contains \"all_text\" (variable used for prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e07415-fbfb-492a-bbee-33e34c24d101",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train ## Contains \"category_encoded\" (variable to be predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe2fc76-d93e-4f08-8045-4c267f27d9b1",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 3: Feature extraction (TF-IDF)\n",
    "\n",
    "### 🔷 What is TF-IDF?\n",
    "\n",
    "TF-IDF (Term Frequency–Inverse Document Frequency) is a widely used technique in text analysis that converts text into **numerical** features based on word importance.\n",
    "\n",
    "Most machine learning algorithms -- including logistic regression, Naive Bayes, and SVM -- cannot directly work with raw text.  \n",
    "Instead, the text must first be converted into a **numerical representation** that captures the relevant features of the input.  \n",
    "TF-IDF is one of the most commonly used methods for this purpose in text classification tasks.\n",
    "\n",
    "\n",
    "- **Term Frequency (TF):** Measures how often a word appears in a document.\n",
    "- **Inverse Document Frequency (IDF):** Downweights words that appear frequently across all documents (e.g., \"and\", \"the\").\n",
    "\n",
    "A high TF-IDF score means the word is frequent in a specific document but rare across the corpus — making it useful for distinguishing meaning. For example, the term \"data scientist\" might appear frequently in one job posting but rarely in others, giving it a high TF-IDF score and making it a strong signal for classification.\n",
    "\n",
    "\n",
    "\n",
    "### 🔷 What I'm doing here\n",
    "\n",
    "In this step, I use `TfidfVectorizer` from `scikit-learn` to transform the input job text (`all_text`) into a matrix of numerical features for model training.\n",
    "\n",
    "- I extract both **unigrams and bigrams** (`ngram_range=(1, 2)`) as features.\n",
    "- I set `max_features=10,000` to retain the top 10,000 most informative tokens.\n",
    "- I apply this transformation separately to the training and test sets, using `.fit_transform()` for training data and `.transform()` for test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd30905-9db4-43ad-b1d8-c47dc230695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    lowercase=True  # default behavior, included for clarity\n",
    ")\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8959dcfb-4abf-4783-97a9-1ba4acd86649",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf  ## Each row = job posting, Each column = unigram or bigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3fa995-4918-4cea-af4e-11313e6d1d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce16aa18-1ab1-44ec-89ea-fb18df383a70",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 4: Logistic regression \n",
    "\n",
    "Logistic regression is a simple method for classification tasks. It models the probability that an input belongs to a particular class.\n",
    "\n",
    "* In this case, it learns the relationship between the **TF-IDF features** (word frequencies) and **job categories.**\n",
    "* The model assigns weights to each word (unigram or bigram), effectively learning which terms are **most predictive** of each job category.\n",
    "* We use multinomial logistic regression since we have more than two job categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f73eb6-be22-4b59-bd1d-8b0b815c13d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train_tfidf, y_train)   ## Training the logistic regression model using the train data \n",
    "    ## => Essentially doing a multinomial logistic regression\n",
    "    ##    where Y-var is the job category \n",
    "    ##    and x_i is the TF-IDF score of a specific unigram or bigram \n",
    "    ##    Fitting 10,000 coefficients \n",
    "y_pred_lr = lr_model.predict(X_test_tfidf)  ## Testing the model on the test data \n",
    "\n",
    "print(\"🔹 Logistic Regression Results:\")\n",
    "print(classification_report(y_test, y_pred_lr, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0763cdca-96a1-4562-b20e-ef8aeebe10e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcb1edb-53b4-4021-826c-dd3c01c8be3e",
   "metadata": {},
   "source": [
    "### 🔷 Comments on the Logistic Regression Results: \n",
    "\n",
    "Logistic regression results are pretty strong. \n",
    "\n",
    "* Precision is around 90%, meaning 90% of all jobs predicted as certain categories are correct.\n",
    "* Recall is also around 80-90%. This means that of all actual jobs in each category, 80-90% are correct.\n",
    "* F1-score is also pretty high across all job categories. \n",
    "  * Note that F1 score is $ F1 \\: score = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall} $\n",
    "  * This balances the trade-off between precision and recall. \n",
    "\n",
    "Overall accuracy is 91%, showing that the model correctly classified 91% of all text examples. \n",
    "\n",
    "Note that because this is the universe of job posting I have for these job categories, I would not use the LR model to predict job category. However, I can use this model to predict job categories for the new job postings in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5a726c-0658-4c47-a2b5-6d75e2e2fd80",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 5: Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "542c93e9-90c1-4f26-b94d-824ff7e3dbdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaive_bayes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultinomialNB\n\u001b[1;32m      5\u001b[0m nb_model \u001b[38;5;241m=\u001b[39m MultinomialNB()\n\u001b[0;32m----> 6\u001b[0m nb_model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train_tfidf\u001b[49m, y_train)\n\u001b[1;32m      7\u001b[0m y_pred_nb \u001b[38;5;241m=\u001b[39m nb_model\u001b[38;5;241m.\u001b[39mpredict(X_test_tfidf)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔹 Naive Bayes Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "## Trying Naive Bayes using job descriptions \n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "y_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"🔹 Naive Bayes Results:\")\n",
    "print(classification_report(y_test, y_pred_nb, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250fe599-dc2c-4fe7-9a0f-7264bb162ea9",
   "metadata": {},
   "source": [
    "### 🔷 Comments on Naive Bayes Results using **Job Descriptions**: \n",
    "\n",
    "Navis Bayes (when using job descriptions) performs worse than logistic regression. This could happen due to \n",
    "\n",
    "1. **Strong (Unrealistic) Assumptions:** Naive Bayes assumes that all words are conditionally independent given the class.\n",
    "But in natural language, that’s not true (e.g. words like \"python\" and \"sql\" offten co-occur). This independence assumption works okay on short texts, but breaks down with longer, richer descriptions.\n",
    "\n",
    "2. **TF-IDF Doesn’t Fit Naive Bayes Perfectly:** Naive Bayes expects raw term frequencies (counts) to estimate probabilities. TF-IDF includes global weights, which distort those probabilities. Logistic Regression handles TF-IDF much better, since it doesn’t rely on probability theory - just feature weights\n",
    "\n",
    "3. **Naive Bayes Struggles with Ambiguous Classes:** \"Consultant\" may appear in job posts that also use words like “business”, “marketing”, “project” → easily confused. Logistic Regression handles correlated features much better.\n",
    "\n",
    "4. **Logistic Regression Learns Interactions More Flexibly:** Logistic regression learns feature weights directly from data. For example, it can learn that \"data\" + \"engineer\" = Data job OR \"project\" + \"manager\" = Project Manager\n",
    "\n",
    "In fact, it is widely knowen that Naives Bayes works better with short documents, few classes, and clean, non-overlapping vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d0e02-0ccc-484e-8ebb-f483980e4919",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trying Naive Bayes only using job titles (instead of job description) \n",
    "\n",
    "## --- STEP 1: Prepare the text data (clean + fill missing titles) \n",
    "jobs_df['title_clean'] = jobs_df['title'].fillna('').str.lower() \n",
    "\n",
    "## --- STEP 2: Train-test split \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    jobs_df['title_clean'],\n",
    "    jobs_df['category_encoded'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=jobs_df['category_encoded']\n",
    ")\n",
    "\n",
    "## --- STEP 3: Feature extraction using CountVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 2))\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "X_test_counts = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caf66d4-965c-4637-bede-d10f4090a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## --- STEP 4: Train and evaluate Naive Bayes \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_counts, y_train)\n",
    "y_pred_nb = nb_model.predict(X_test_counts)\n",
    "\n",
    "print(\"🔹 Naive Bayes (using titles only) Results:\")\n",
    "print(classification_report(y_test, y_pred_nb, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ef1454-0c7a-4802-a2ee-9882d391199b",
   "metadata": {},
   "source": [
    "### 🔷 Comments on Naive Bayes Results using **Job Titles**:  \n",
    "\n",
    "Interestingly, Naive Bayes performs **much better** when using job titles, instead of job descriptions. This approach also outperforms logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde5c28f-61d5-4c49-ac99-25d24b12dfdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py310-nlp]",
   "language": "python",
   "name": "conda-env-py310-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
