{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d037058a-8828-470d-ad11-20191e5a7d13",
   "metadata": {},
   "source": [
    "# 1-2. Categorize job titles using supervised learning approach \n",
    "\n",
    "Job posting data is provided by: https://www.kaggle.com/datasets/arshkon/linkedin-job-postings\n",
    "\n",
    "Author: Yu Kyung Koh\n",
    "\n",
    "Last Updated: 2025/05/18\n",
    "\n",
    "In this code, I apply supervised learning methods (e.g., logistic regression, Naive Bayes) to classify jobs.\n",
    "\n",
    "IMPORTANT: This exercise is primarily intended to help myself become more familiar with commonly used machine learning techniques. I do not use these methods for the actual job classification in this project, as I believe the rule-based approach is sufficient for now.\n",
    "\n",
    "However, if I collect more job postings in the future, the supervised learning approach developed here could be useful for classifying newly gathered, unlabeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60407a75-6cdc-4f55-aeb0-0178ff089664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcf4a30-c074-480f-bd52-a14fa50a0650",
   "metadata": {},
   "source": [
    "## SECTION 1: Prepare the job posting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4448a51-fda5-4e74-80e0-b7a176a1b53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       job_id                             company_name  \\\n",
      "0      921716                    Corcoran Sawyer Smith   \n",
      "2    10998357                   The National Exemplar    \n",
      "12   56482768                                      NaN   \n",
      "14   69333422                          Staffing Theory   \n",
      "18  111513530  United Methodists of Greater New Jersey   \n",
      "\n",
      "                                            title  work_type  \\\n",
      "0                           Marketing Coordinator  FULL_TIME   \n",
      "2                     Assitant Restaurant Manager  FULL_TIME   \n",
      "12  Appalachian Highlands Women's Business Center  FULL_TIME   \n",
      "14               Senior Product Marketing Manager  FULL_TIME   \n",
      "18                 Content Writer, Communications  FULL_TIME   \n",
      "\n",
      "    normalized_salary                                      combined_desc  \\\n",
      "0             38480.0  Job descriptionA leading real estate firm in N...   \n",
      "2             55000.0  The National Exemplar is accepting application...   \n",
      "12                NaN  FULL JOB DESCRIPTION – PROGRAM DIRECTOR Appala...   \n",
      "14                NaN  A leading pharmaceutical company committed to ...   \n",
      "18                NaN  Application opening date: April 24, 2024\\nTitl...   \n",
      "\n",
      "            job_category  \n",
      "0              Marketing  \n",
      "2          Other Manager  \n",
      "12  Business/Finance Job  \n",
      "14             Marketing  \n",
      "18             Marketing  \n"
     ]
    }
   ],
   "source": [
    "## Import cleaned data from code 1-1\n",
    "cleandatadir = '/Users/yukyungkoh/Desktop/1_Post-PhD/7_Python-projects/2_practice-NLP_job-posting_NEW/2_data/cleaned_data'\n",
    "jobdata = os.path.join(cleandatadir, '1_job-posting_jobs-categorized_df.pkl')\n",
    "jobs_df = pd.read_pickle(jobdata, 'zip')\n",
    "print(jobs_df.head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3dd51de-901b-434b-97ef-bd7eb2760bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing\n",
    "jobs_df = jobs_df.dropna(subset=['combined_desc', 'job_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad3208c0-6ea6-47b3-bc88-23a3787d6644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       job_id                             company_name  \\\n",
      "0      921716                    Corcoran Sawyer Smith   \n",
      "2    10998357                   The National Exemplar    \n",
      "12   56482768                                      NaN   \n",
      "14   69333422                          Staffing Theory   \n",
      "18  111513530  United Methodists of Greater New Jersey   \n",
      "\n",
      "                                            title  work_type  \\\n",
      "0                           Marketing Coordinator  FULL_TIME   \n",
      "2                     Assitant Restaurant Manager  FULL_TIME   \n",
      "12  Appalachian Highlands Women's Business Center  FULL_TIME   \n",
      "14               Senior Product Marketing Manager  FULL_TIME   \n",
      "18                 Content Writer, Communications  FULL_TIME   \n",
      "\n",
      "    normalized_salary                                      combined_desc  \\\n",
      "0             38480.0  Job descriptionA leading real estate firm in N...   \n",
      "2             55000.0  The National Exemplar is accepting application...   \n",
      "12                NaN  FULL JOB DESCRIPTION – PROGRAM DIRECTOR Appala...   \n",
      "14                NaN  A leading pharmaceutical company committed to ...   \n",
      "18                NaN  Application opening date: April 24, 2024\\nTitl...   \n",
      "\n",
      "            job_category  category_encoded  \n",
      "0              Marketing                 3  \n",
      "2          Other Manager                 4  \n",
      "12  Business/Finance Job                 0  \n",
      "14             Marketing                 3  \n",
      "18             Marketing                 3  \n"
     ]
    }
   ],
   "source": [
    "# Encode target labels\n",
    "#  => This convert the category labels from text to numeric form, which is required by most scikit-learn models \n",
    "\n",
    "le = LabelEncoder()\n",
    "jobs_df['category_encoded'] = le.fit_transform(jobs_df['job_category'])\n",
    "print(jobs_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8662f18-b7b7-4fc6-8c14-ddeb9a767e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_encoded\n",
      "4    13385\n",
      "0     4557\n",
      "5     3400\n",
      "3     2598\n",
      "6     1994\n",
      "2     1946\n",
      "1     1844\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Preview category\n",
    "print(jobs_df[\"category_encoded\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "132b87ae-fb80-4254-8915-869c5ab062a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine job title and job description \n",
    "\n",
    "jobs_df['all_text'] = jobs_df['title'].fillna('') + ' ' + jobs_df['combined_desc'].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21cb5ad-9fda-4111-8d49-0725c76bf2ee",
   "metadata": {},
   "source": [
    "## SECTION 2: Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16929824-3199-4651-9fca-794f5208b311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    jobs_df['all_text'], \n",
    "    jobs_df['category_encoded'], \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=jobs_df['category_encoded']  ## Keeps category proportions the same in train/test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4767c8cc-262c-4624-9fdb-ef8e5668f53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train ## Contains \"all_text\" (variable used for prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e07415-fbfb-492a-bbee-33e34c24d101",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train ## Contains \"category_encoded\" (variable to be predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe2fc76-d93e-4f08-8045-4c267f27d9b1",
   "metadata": {},
   "source": [
    "## SECTION 3: Feature extraction (TF-IDF)\n",
    "\n",
    "What TfidVectorizer does: \n",
    "\n",
    "1. Tokenization\n",
    "2. Stopword removal\n",
    "3. TF-IDF Scoring\n",
    "   * Calculates the importance of each term in a document\n",
    "   * High score = word is frequent in this document but rare overall \n",
    "5. Vectorization\n",
    "   * Each document becomes a row in a sparse matrix, with one column per word/ngram.\n",
    "   * If I set max_features=10000, only the top 10,000 most important tokens (across all documents) are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd30905-9db4-43ad-b1d8-c47dc230695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    lowercase=True  # default behavior, included for clarity\n",
    ")\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8959dcfb-4abf-4783-97a9-1ba4acd86649",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf  ## Each row = job posting, Each column = unigram or bigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3fa995-4918-4cea-af4e-11313e6d1d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce16aa18-1ab1-44ec-89ea-fb18df383a70",
   "metadata": {},
   "source": [
    "## SECTION 4: Logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f73eb6-be22-4b59-bd1d-8b0b815c13d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train_tfidf, y_train)   ## Training the logistic regression model using the train data \n",
    "    ## => Essentially doing a multinomial logistic regression\n",
    "    ##    where Y-var is the job category \n",
    "    ##    and x_i is the TF-IDF score of a specific unigram or bigram \n",
    "    ##    Fitting 10,000 coefficients \n",
    "y_pred_lr = lr_model.predict(X_test_tfidf)  ## Testing the model on the test data \n",
    "\n",
    "print(\"🔹 Logistic Regression Results:\")\n",
    "print(classification_report(y_test, y_pred_lr, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0763cdca-96a1-4562-b20e-ef8aeebe10e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcb1edb-53b4-4021-826c-dd3c01c8be3e",
   "metadata": {},
   "source": [
    "### Comment on the Logistic Regression Results: \n",
    "\n",
    "Logistic regression results are pretty strong. \n",
    "\n",
    "* Precision is around 90%, meaning 90% of all jobs predicted as certain categories are correct.\n",
    "* Recall is also around 80-90%. This means that of all actual jobs in each category, 80-90% are correct.\n",
    "* F1-score is also pretty high across all job categories. \n",
    "  * Note that F1 score is $ F1 \\: score = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall} $\n",
    "  * This balances the trade-off between precision and recall. \n",
    "\n",
    "Overall accuracy is 91%, showing that the model correctly classified 91% of all text examples. \n",
    "\n",
    "Note that because this is the universe of job posting I have for these job categories, I would not use the LR model to predict job category. However, I can use this model to predict job categories for the new job postings in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5a726c-0658-4c47-a2b5-6d75e2e2fd80",
   "metadata": {},
   "source": [
    "## SECTION 5: Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542c93e9-90c1-4f26-b94d-824ff7e3dbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trying Naive Bayes using job descriptions \n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "y_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"🔹 Naive Bayes Results:\")\n",
    "print(classification_report(y_test, y_pred_nb, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250fe599-dc2c-4fe7-9a0f-7264bb162ea9",
   "metadata": {},
   "source": [
    "### Comments on Naive Bayes Results using Job Descriptions: \n",
    "\n",
    "Navis Bayes (when using job descriptions) performs worse than logistic regression. This could happen due to \n",
    "\n",
    "1. **Strong (Unrealistic) Assumptions:** Naive Bayes assumes that all words are conditionally independent given the class.\n",
    "But in natural language, that’s not true (e.g. words like \"python\" and \"sql\" offten co-occur). This independence assumption works okay on short texts, but breaks down with longer, richer descriptions.\n",
    "\n",
    "2. **TF-IDF Doesn’t Fit Naive Bayes Perfectly:** Naive Bayes expects raw term frequencies (counts) to estimate probabilities. TF-IDF includes global weights, which distort those probabilities. Logistic Regression handles TF-IDF much better, since it doesn’t rely on probability theory - just feature weights\n",
    "\n",
    "3. **Naive Bayes Struggles with Ambiguous Classes:** \"Consultant\" may appear in job posts that also use words like “business”, “marketing”, “project” → easily confused. Logistic Regression handles correlated features much better.\n",
    "\n",
    "4. **Logistic Regression Learns Interactions More Flexibly:** Logistic regression learns feature weights directly from data. For example, it can learn that \"data\" + \"engineer\" = Data job OR \"project\" + \"manager\" = Project Manager\n",
    "\n",
    "In fact, it is widely knowen that Naives Bayes works better with short documents, few classes, and clean, non-overlapping vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d0e02-0ccc-484e-8ebb-f483980e4919",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trying Naive Bayes only using job titles (instead of job description) \n",
    "\n",
    "## --- STEP 1: Prepare the text data (clean + fill missing titles) \n",
    "jobs_df['title_clean'] = jobs_df['title'].fillna('').str.lower() \n",
    "\n",
    "## --- STEP 2: Train-test split \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    jobs_df['title_clean'],\n",
    "    jobs_df['category_encoded'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=jobs_df['category_encoded']\n",
    ")\n",
    "\n",
    "## --- STEP 3: Feature extraction using CountVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 2))\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "X_test_counts = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caf66d4-965c-4637-bede-d10f4090a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## --- STEP 4: Train and evaluate Naive Bayes \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_counts, y_train)\n",
    "y_pred_nb = nb_model.predict(X_test_counts)\n",
    "\n",
    "print(\"🔹 Naive Bayes (using titles only) Results:\")\n",
    "print(classification_report(y_test, y_pred_nb, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ef1454-0c7a-4802-a2ee-9882d391199b",
   "metadata": {},
   "source": [
    "### Comments on Naive Bayes Results using Job Titles:  \n",
    "\n",
    "Naive Bayes performs **much better** when using job titles, instead of job descriptions. In fact, this approach outperforms logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d191996c-e97a-4185-8319-a579c2964f20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py310-nlp]",
   "language": "python",
   "name": "conda-env-py310-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
