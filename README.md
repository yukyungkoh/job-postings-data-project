# Job Postings Data Project

This repository contains a collection of Jupyter notebooks and scripts aimed at learning how to apply **natural language processing (NLP)** and **large language models (LLMs)** to extract structured insights from messy, unstructured **job postings**. Using a sample of 100,000+ job ads, the project explores methods for leveraging job posting data to study the labor market in real time. 


## Project Structure

The project is organized into the following parts:

---

### ðŸ”· Part 1: Categorize Job Titles  
This section explores multiple NLP approaches for classifying job titles into categories, including:
- Rule-based methods
- Supervised learning (e.g., logistic regression, Naive Bayes)
- Unsupervised learning (e.g., k-means clustering, LDA topic modeling)

[Browse the notebook folder for Part 1](https://github.com/yukyungkoh/job-postings-data-project/tree/main/1_categorize-job-titles)

---

### ðŸ”· Part 2: Extract Job Skills and Tasks  
This section focuses on extracting job skills and task descriptions from job posting text using:
- Keyword matching to a curated skill list (from Lightcast)
- Sentence embeddings
- Large language models (LLMs)

[Browse the notebook folder for Part 2](https://github.com/yukyungkoh/job-postings-data-project/tree/main/2_extract-job-skills-and-tasks)

---

### ðŸ”· Part 3: Measure AI Demand Exposure  
This section estimates the **AI exposure** of job tasks -- i.e., how susceptible the listed tasks in job postings are to automation or augmentation by AI tools. It builds on task-level extraction to compute AI relevance scores.

[Browse the notebook folder for Part 3](https://github.com/yukyungkoh/job-postings-data-project/tree/main/3_measure-AI-demand-exposure)


---

**Author:** Yu Kyung Koh 

**Last updated:** July 4, 2025