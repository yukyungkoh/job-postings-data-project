{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdd08ed2-e991-4aaa-aa6f-d0b6dc718882",
   "metadata": {},
   "source": [
    "# üè∑Ô∏è Part 2.3 - Extract job skills using LLMs\n",
    "\n",
    "**Author:** Yu Kyung Koh  \n",
    "**Last Updated:** July 13, 2025  \n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Objective\n",
    "\n",
    "* Extract job skills from job postings using LLM\n",
    "* Specifically, I use **Mistral** model via Ollama, which is free to use and fairly high-performing among the free versions. \n",
    "* To ensure consistency across extracted skill terms (e.g., \"Microsoft Office\" vs. \"Microsoft Office Suite\"), I apply a **harmonization procedure** that clusters semantically similar skills using **sentence embeddings** and **unsupervised clustering.**\n",
    "  \n",
    "### üóÇÔ∏è Outline\n",
    "* **Section 1:** Bring in the job posting data\n",
    "* **Section 2:** Extract skills using the Mistral model via Ollama\n",
    "* **Section 3:** Harmonize similar skill terms using embedding + clustering\n",
    "* **Section 4:** Visualize extracted skills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b868a754-606a-419b-8238-579e5a6dd1da",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 1: Bring in the job posting data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e0a12e-e013-410f-a600-d49f11b72a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#from rapidfuzz import process, fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327fe73d-28b4-4b53-b322-05aba0189af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# STEP 1: Import data\n",
    "# --------------------------------------\n",
    "datadir = '../data/'\n",
    "jobposting_file = os.path.join(datadir, 'synthetic_job_postings_combined.csv')\n",
    "\n",
    "posting_df = pd.read_csv(jobposting_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b9819e-38b5-4051-bb5d-edfcd9e8a5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "posting_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7b3935-772a-4a81-aad6-52d375e16554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many job postings are in this data \n",
    "len(posting_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b14cef-2ecd-4849-adba-589a4f885d80",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 2: Extract skills using the Mistral model via Ollama\n",
    "\n",
    "* Before running below, we need to type `ollama run mistral` in the terminal\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71457479-418b-4885-aa18-92315fefc8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# STEP 1: Extract skills using the Mistral model\n",
    "# --------------------------------------\n",
    "from ollama import chat\n",
    "\n",
    "# Limit to the first 200 job postings\n",
    "sample_posting_df = posting_df.head(200).copy()\n",
    "\n",
    "### Initialize list for storing results\n",
    "extracted_skills_mistral = []\n",
    "\n",
    "### Loop through job postings in existing results_df\n",
    "for desc in tqdm(sample_posting_df[\"posting_text\"]):\n",
    "    prompt = f\"\"\"Extract all job **skills** required in the following job posting.\n",
    "            Return them as a comma-separated list of keywords only (e.g., Python, Excel, Project Management).\n",
    "            Include both technical and soft skills.\n",
    "            Do not include:\n",
    "                - Job titles (e.g., Educational Program Coordinator)\n",
    "                - DEI-related terms (e.g., Diversity, Inclusion)\n",
    "                - Qualifiers like \"proficiency\", \"ability\", \"skills\", or \"experience with\"\n",
    "                - Descriptions or explanations ‚Äî only the canonical skill names\n",
    "\n",
    "            Job posting:\n",
    "            \\\"\\\"\\\"{desc}\\\"\\\"\\\"\n",
    "            \"\"\"\n",
    "    response = chat(model='mistral', messages=[\n",
    "        {'role': 'user', 'content': prompt}\n",
    "    ])\n",
    "    \n",
    "    extracted = response['message']['content']\n",
    "    extracted_skills_mistral.append(extracted)\n",
    "\n",
    "### Add new column to results_df\n",
    "sample_posting_df[\"extracted_skills_mistral\"] = extracted_skills_mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3b47a3-c9cd-4860-a95f-5a2883c7a6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# STEP 2: Examine extracted skills\n",
    "# --------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2812506-d48f-44da-bfd9-18ebaaad14e1",
   "metadata": {},
   "source": [
    "### üî∑ Comments\n",
    "\n",
    "* Initial results suggest that the LLM is reasonably effective at extracting job skills from postings.\n",
    "* However, there are two important caveats:\n",
    "\n",
    "  1. **Performance and Scalability**\n",
    "     * Extraction is time-consuming ‚Äî processing 100 job postings took over 10 minutes.\n",
    "     * Scaling this to millions of postings may be infeasible with the current setup.\n",
    "     * A practical alternative for large datasets is to **combine LLMs with machine learning**:\n",
    "       - Use the LLM to label skill phrases on a small subset of job postings.\n",
    "       - Train a supervised skill extraction model using these labeled examples.\n",
    "\n",
    "  2. **Inconsistent Skill Terminology**\n",
    "     * The same skill can appear under different names across postings (e.g., *Microsoft Office* vs. *Microsoft Office Suite*).\n",
    "     * To address this, I apply **skill harmonization using embeddings and clustering**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c36b5c7-ac51-4a5f-8773-4524a4cee9a2",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Harmonize similar skill terms using embedding + clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ffcd01-1158-42e9-bf5f-61817c25933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# STEP 1: Parse extracted_skills_mistral into a flat skill list \n",
    "# --------------------------------------\n",
    "import pandas as pd\n",
    "\n",
    "# Safely split and normalize the extracted skills\n",
    "sample_posting_df[\"parsed_skills\"] = sample_posting_df[\"extracted_skills_mistral\"].apply(\n",
    "    lambda x: [s.strip().lower() for s in x.split(\",\")] if isinstance(x, str) else []\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3681881b-cb9b-43b7-a6e7-03346b7d9da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# STEP 2: Embed all skills\n",
    "# --------------------------------------\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Flatten and lowercase all skills before embedding\n",
    "all_skills = sorted(set(skill.strip().lower() for skills in sample_posting_df[\"parsed_skills\"] for skill in skills))\n",
    "\n",
    "# Get embeddings\n",
    "embeddings = model.encode(all_skills, convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186684d2-a161-4e61-bbd5-cefb8768a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# STEP 3: Cluster skills using Agglomerative clustering\n",
    "# --------------------------------------\n",
    "\n",
    "# Cluster similar skills\n",
    "clustering = AgglomerativeClustering(\n",
    "    n_clusters=None,\n",
    "    distance_threshold=0.3,  # try between 0.2‚Äì0.4\n",
    "    linkage='average',\n",
    "    metric='cosine'\n",
    ")\n",
    "labels = clustering.fit_predict(embeddings)\n",
    "\n",
    "\n",
    "# Create mapping: label ‚Üí canonical skill (e.g., the shortest skill in group)\n",
    "cluster_map = {}\n",
    "for label in set(labels):\n",
    "    cluster_skills = [s for s, l in zip(all_skills, labels) if l == label]\n",
    "    if not cluster_skills:\n",
    "        continue  # skip empty clusters\n",
    "    canonical = Counter(cluster_skills).most_common(1)[0][0]  # most frequent\n",
    "    for s in cluster_skills:\n",
    "        cluster_map[s] = canonical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee62627-3264-476d-9bb9-7a991ee6b493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# STEP 4: Replace original parsed skills with harmonized version\n",
    "# --------------------------------------\n",
    "def harmonize_skills(skill_list):\n",
    "    return list(set(cluster_map.get(s, s) for s in skill_list))\n",
    "\n",
    "sample_posting_df[\"harmonized_skills\"] = sample_posting_df[\"parsed_skills\"].apply(harmonize_skills)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a222fe4-6b1d-45ba-9ef6-dcba0b7fb595",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Visualize extracted skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0489120e-4ebb-4ae5-8a27-1d6f1457e2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# STEP 1: Combine skills by sector\n",
    "# --------------------------------------\n",
    "from collections import defaultdict\n",
    "\n",
    "# Create a dictionary to hold all skills per sector\n",
    "sector_skills = defaultdict(list)\n",
    "\n",
    "for _, row in sample_posting_df.iterrows():\n",
    "    sector = row[\"sector\"]\n",
    "    skills = row[\"harmonized_skills\"]\n",
    "    if isinstance(skills, list):  # skip NaNs or non-lists\n",
    "        sector_skills[sector].extend(skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e71adde-05b4-4208-82ca-c1af833a4560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# STEP 2: Generate WordClouds per sector\n",
    "# --------------------------------------\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for sector, skills in sector_skills.items():\n",
    "    text = \" \".join(skills)\n",
    "\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(text)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Most common skills (LLM-extracted): Sector {sector} \", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py310-nlp]",
   "language": "python",
   "name": "conda-env-py310-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
